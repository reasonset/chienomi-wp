<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="ja" xml:lang="ja">
  <head>
    <meta charset="utf-8" />
    <meta name="generator" content="PureBuilder Simply WP Importer" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
    <meta name="author" content="Masaki Haruka" />
    <meta name="date" content="2015-02-02T00:00:00+09:00" />
    <meta name="dcterms.date" content="2015-02-02T00:00:00+09:00" />
    <title>新しいストレージワーク(GlusterFS, AoE, lsyncd) - Chienomi</title>
    <style type="text/css">article {
max-width: 1080px;
margin: auto;
}</style>
  </head>
  <body>

    <section id="ContentContainer">
      <header id="MainHeader"><a href="/"><h1>Chienomi</h1></a></header>
      <section id="ArticleBox">
        <header id="ArticleTitle"><h1>新しいストレージワーク(GlusterFS, AoE, lsyncd)</h1></header>
        <article id="MainArticle">

	<h3>問題点</h3>
	<p>分かってはいたことだが、先日の件でbtrfs mirrorでは不十分だ。データの冗長化は次のようなことが考えられる。</p>
	<ul>
		<li>ストレージの故障</li>
		<li>ファイルシステムの故障</li>
		<li>RAIDハードウェアの故障</li>
		<li>誤操作による喪失</li>
		<li>コンピュータノードの故障</li>
		<li>コンピュータノードの喪失（災害や盗難など）</li>
	</ul>
	<p>基本的な対応は次の通り。</p>
	<dl>
		<dt>ストレージの故障</dt>
		<dd>ミラーリングを行い、故障したストレージを置き換える</dd>

		<dt>ファイルシステムの故障</dt>
		<dd>異なるファイルシステム間でミラーするか、バックアップする。</dd>

		<dt>RAIDハードウェアの故障</dt>
		<dd>同一のRAIDハードウェアに再接続するか、バックアップによって復元する</dd>

		<dt>誤操作</dt>
		<dd>gitやpdumpfs、あるいはLogFSなどスナップショットの活用</dd>

		<dt>ノードの故障</dt>
		<dd>新規ノードにストレージを移設する。可用性が不要ならばデータの冗長化は不要</dd>

		<dt>ノードの喪失</dt>
		<dd>可用性の損失は避けられない。同一箇所に保管していると同時に損失する可能性は高いため、クラウドバックアップが有効</dd>
	</dl>
	<p>一般にノードの損失は稀なケースだと考えられる。だが、実際はそこそこ可能性はある。落雷によっても起こりうる。この場合のことを考えれば、データを諦められないのであればバックアップは必要だ。</p>
	<p>これまでは、btrfsの2レッグミラーを採用していた。これはストレージの故障に対する冗長性・可用性を担保する。しかし、それ以外には対応できていない。特に怖いのは、未だexperimentalであるbtrfs自体の異常だ。ファイルシステムが壊れてしまえばミラーされていたものも含めてすべてのデータがアクセス不能になる。</p>
	<p>少々複雑な話に聞こえるかもしれないが、btrfsは「壊れにくいファイルシステム」だ。日常的な読み書きで起こるビットエラーをbtrfsは発見し、ミラーリングを行っていれば訂正することすら可能だ。そのため、他のファイルシステムと比べ非常に壊れにくい。それは他のジャーナリングファイルシステムと比べてもだ。そのためた信頼性が高いと言われ、この点を買って私は採用している。一方で、btrfsはまだ完成度が低く、btrfs自体が壊れる挙動を示す可能性がある。そのため信頼性が低いとされている。</p>
	<p>データが既に2TiBを超えている現状にあっては、バックアップやその復元も容易ではない。ちなみに、この次にハードルが上がるのは、ストレージ1台ではすまなくなる時だ。コストを度外視すれば、8TBということになる。</p>
	<p>基本的にはバックアップというよりも、ミラーリングによって冗長性を持たせ、データが損失しないことを前提とする方向としている。今後も情報増加は進む予定だし、それに対応する構成としておかなくてはならないからだ。そして、データ量は一般的なシングルストレージで済む量に収まる見込みはない。ストレージ増加よりもデータ増加の方がはるかに速い。</p>
	<p>現在の対応は次の通りだ。</p>
	<ul>
		<li>btrfsによるジャーナリング、自動訂正によるビットエラー対策</li>
		<li>btrfsミラーによる冗長化</li>
		<li>btrfsによる柔軟なストレージ追加。1台単位でディスク容量を問わず追加し任意のボリュームを切り出せる</li>
		<li>必要な箇所をgitとすることで誤操作による損失の防止</li>
		<li>一部データのリモートgitへのclone</li>
	</ul>
	<p>だが、先日はそのストレージを格納するコンピュータノードがダウンした。そのため、データ自体にアクセスできなくなった。現在でも最低限、4台のストレージを搭載しなければデータを使うことができない。Proliant Microserverは搭載ストレージ台数は4なので、システムドライブを含めるとそれを搭載することができない。</p>
	<p>もちろん、データは損失していないのだからコンピュータノードを追加すれば良いのだが、その間仕事は完全に停止してしまう。やはりこの点も冗長性が必要だろう。</p>
	<p>機能問題は別として、別のコンピュータがデータを触れれば良いのだ。コンピュータノード全体がダウンしても機能するようにすれば、1台のコンピュータが損失しても問題ない、ということになる。もっとも、実際に必要となるのは完全なクラウドバックアップだが、現在のところそれは月額で8万円程度が必要になるため、現実的でない。</p>
	<h3>GlusterFS</h3>
	<p>GlusterFSはペタバイト規模に対応するクラスタストレージ技術だ。ユーザースペースで動作するデーモンであり、大きなファイルの読み書きはネイティブなコードで行う。FUSEでマウントできるだけでなく、NFSやCIFSでもマウント可能。</p>
	<p>分散ファイルシステムだがネイティブファイルシステムではなく、各コンピュータはマウントされた特定のディレクトリをGlusterFSデーモンによって公開する（blickと呼ぶ）。クライアントはblickを束ねてvolumeとしてマウントすることができる。中央サーバーがないのがGlusterFSの特徴だ。</p>
	<p>CephやGFSと比べかなりお手軽に使えそうに見えるGlusterFSだが、実際には意外と厳しい。それは、blickの容量をみずにファイルベースで振り分けていくこと、GlusterFSのレプリケーションはblickを組みとして複製するものであることによる。このことから、実質的にはblickはすべて同一容量でなくてはならない。しかも、blickの追加はRAID化している数をunitとして行わなくてはいけない。例えばstriped replicated volumeだとしたら、4 blicksが追加単位となるのだ。ディスク単位のblickにしても4台のディスク、コンピュータ単位だと4台のコンピュータだ！</p>
	<p>また、このような技術はノードの停止は「障害」であるため、HAの理屈に基づき稼働停止が許されなくなる。電気代で考えても厳しいし、デスクトップユースでの無停止はそもそも難しい。結構よく見えたのだが、GlusterFS適用はかなり難しいように見えた。</p>
	<p>ただし、適用方法はある。それは、「そもそも2組のストレージにしてしまい、2 blicks GlusterFS volumeにする」という方法だ。例えばiSCSI+LVMなどで複数のコンピュータノードからなるひとつのファイルシステムを編成し、それをマウントし、それをblickにすれば良い。そうすればコンピュータを2組に分けることができ、片方の組のコンピュータが停止しても全体はダウンしない。ストレージ追加はLVMなどの単位で行えるためかなり柔軟だ。容量の問題は、単に合計容量が小さいほうの組を上限としてそれを越えないように運用する、という制約があるだけだ。</p>
	<h3>lsyncd</h3>
	<p>だが、やはりこのようなHA技術は少々重い。それであれば少なくともデスクトップは使う側にしてデスクトップにストレージをもたせるのはやめるべきだ。</p>
	<p>任意に停止しゆるい同期を行えれば良い。つまり、「手動で同期サービスを開始・停止し、ファイル更新時に反映してくれれば良い」という考えだ。可用性については、短時間の停止なら十分許容できる。</p>
	<p>原始的には定期的にrsyncを回す方法もあるが、せめてファイル更新を監視したい。別にそれをスクリプトにしてもいいのだが、ionotifyというLinuxの機能があるのだから、それを活かしたいもの。そこで「ファイル変更を監視して同期する」というものを探したところ（正確には「なんて名前だっけ」だった）、lsyncdが見つかった。</p>
	<p>lsyncdはファイル同期デーモンだが、実際にはミラーリングを行う機能はなく、rsyncなどをバックエンドとして使う。つまり、lsysncdはionotifyによってファイル更新を受け取り、それをトリガーとしてrsyncなどを起動するデーモンだ。</p>
	<p>デスクトップレベルではこれが順当なところではないだろうか。台数が増えるとオンオフも困難になるので普通にHAストレージでいいのかもしれないが。</p>
	<p>台数が少ないうちは普通にイーサネットケーブルでつなぎ、それをまとめて2つのストレージを編成すればいいのだが、台数が増えてきた場合、それぞれキーになるノード（片方はデスクトップ）をシングルにつなぎ、それぞれがストレージ用ハブを持っておくと良いだろう。構成手順は次のとおり</p>
	<ul>
		<li>デスクトップは現在btrfsミラー</li>
		<li>キーサーバーAを接続し、ZFSでストレージを束ねる</li>
		<li>キーサーバーAにrsyncで全データをバックアップ</li>
		<li>デスクトップのbtrfsのミラーをやめて構成しなおす</li>
		<li>キーサーバーAからrsyncでデスクトップにデータを戻す</li>
		<li>デスクトップとキーサーバーAにGbEインターフェイスを追加する</li>
		<li>追加されたGbEインターフェイスをハブに接続する</li>
		<li>ストレージサーバーノードをGbEでストレージ側ハブに接続する</li>
		<li>ストレージサーバーノードのディスクをAoEを用いてそれぞれのキーノードのブロックデバイスとしてみえるようにする</li>
		<li>追加されたストレージサーバーノードのディスク(AoE)をbtrfs/ZFSノードに加える</li>
		<li>btrfs/ZFSをリバランス</li>
	</ul>
	<p>10GbEに置き換えるところまで考えればかなりの規模までこれでいけるように思う。ただし、グループノードのいずれかがダウンすると障害発生なので、「故障率」は当然あがる。これはグリッドシステムでは常に起こることだ。</p>
<!-- PBSEARCH_RESULT -->
        </article>
    </section>
    <footer id="InfoFooter">
      <ul>
        <li>© Masaki Haruka 2003.</li>
        <li>Generated by <a href="https://gitlab.com/reasonset/pbsimply-wpimporter">PureBuilder Simply WP Importer</a></li>
      </ul>
    </footer>
  </body>
</html>
